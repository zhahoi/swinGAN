{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2022-04-19T01:52:37.357917Z","iopub.status.busy":"2022-04-19T01:52:37.357430Z","iopub.status.idle":"2022-04-19T01:52:38.807179Z","shell.execute_reply":"2022-04-19T01:52:38.806469Z","shell.execute_reply.started":"2022-04-19T01:52:37.357832Z"},"trusted":true},"outputs":[],"source":["# Differentiable Augmentation for Data-Efficient GAN Training\n","# Shengyu Zhao, Zhijian Liu, Ji Lin, Jun-Yan Zhu, and Song Han\n","# https://arxiv.org/pdf/2006.10738\n","\n","import torch\n","import torch.nn.functional as F\n","import random\n","\n","def DiffAugment(x, policy='', channels_first=True):\n","    if policy:\n","        if not channels_first:\n","            x = x.permute(0, 3, 1, 2)\n","        for p in policy.split(','):\n","            for f in AUGMENT_FNS[p]:\n","                x = f(x)\n","        if not channels_first:\n","            x = x.permute(0, 2, 3, 1)\n","        x = x.contiguous()\n","    return x\n","\n","\n","def rand_brightness(x):\n","    x = x + (torch.rand(x.size(0), 1, 1, 1, dtype=x.dtype, device=x.device) - 0.5)\n","    return x\n","\n","\n","def rand_saturation(x):\n","    x_mean = x.mean(dim=1, keepdim=True)\n","    x = (x - x_mean) * (torch.rand(x.size(0), 1, 1, 1, dtype=x.dtype, device=x.device) * 2) + x_mean\n","    return x\n","\n","\n","def rand_contrast(x):\n","    x_mean = x.mean(dim=[1, 2, 3], keepdim=True)\n","    x = (x - x_mean) * (torch.rand(x.size(0), 1, 1, 1, dtype=x.dtype, device=x.device) + 0.5) + x_mean\n","    return x\n","\n","\n","def rand_translation(x, ratio=0.2):\n","    shift_x, shift_y = int(x.size(2) * ratio + 0.5), int(x.size(3) * ratio + 0.5)\n","    translation_x = torch.randint(-shift_x, shift_x + 1, size=[x.size(0), 1, 1], device=x.device)\n","    translation_y = torch.randint(-shift_y, shift_y + 1, size=[x.size(0), 1, 1], device=x.device)\n","    grid_batch, grid_x, grid_y = torch.meshgrid(\n","        torch.arange(x.size(0), dtype=torch.long, device=x.device),\n","        torch.arange(x.size(2), dtype=torch.long, device=x.device),\n","        torch.arange(x.size(3), dtype=torch.long, device=x.device),\n","    )\n","    grid_x = torch.clamp(grid_x + translation_x + 1, 0, x.size(2) + 1)\n","    grid_y = torch.clamp(grid_y + translation_y + 1, 0, x.size(3) + 1)\n","    x_pad = F.pad(x, [1, 1, 1, 1, 0, 0, 0, 0])\n","    x = x_pad.permute(0, 2, 3, 1).contiguous()[grid_batch, grid_x, grid_y].permute(0, 3, 1, 2)\n","    return x\n","\n","\n","def rand_cutout(x, ratio=0.5):\n","    if random.random() < 0.3:\n","        cutout_size = int(x.size(2) * ratio + 0.5), int(x.size(3) * ratio + 0.5)\n","        offset_x = torch.randint(0, x.size(2) + (1 - cutout_size[0] % 2), size=[x.size(0), 1, 1], device=x.device)\n","        offset_y = torch.randint(0, x.size(3) + (1 - cutout_size[1] % 2), size=[x.size(0), 1, 1], device=x.device)\n","        grid_batch, grid_x, grid_y = torch.meshgrid(\n","            torch.arange(x.size(0), dtype=torch.long, device=x.device),\n","            torch.arange(cutout_size[0], dtype=torch.long, device=x.device),\n","            torch.arange(cutout_size[1], dtype=torch.long, device=x.device),\n","        )\n","        grid_x = torch.clamp(grid_x + offset_x - cutout_size[0] // 2, min=0, max=x.size(2) - 1)\n","        grid_y = torch.clamp(grid_y + offset_y - cutout_size[1] // 2, min=0, max=x.size(3) - 1)\n","        mask = torch.ones(x.size(0), x.size(2), x.size(3), dtype=x.dtype, device=x.device)\n","        mask[grid_batch, grid_x, grid_y] = 0\n","        x = x * mask.unsqueeze(1)\n","    return x\n","\n","def rand_rotate(x, ratio=0.5):\n","    k = random.randint(1,3)\n","    if random.random() < ratio:\n","        x = torch.rot90(x, k, [2,3])\n","    return x\n","\n","AUGMENT_FNS = {\n","    'color': [rand_brightness, rand_saturation, rand_contrast],\n","    'translation': [rand_translation],\n","    'cutout': [rand_cutout],\n","    'rotate': [rand_rotate],\n","}"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2022-04-19T01:52:38.809751Z","iopub.status.busy":"2022-04-19T01:52:38.809353Z","iopub.status.idle":"2022-04-19T01:52:59.930014Z","shell.execute_reply":"2022-04-19T01:52:59.929196Z","shell.execute_reply.started":"2022-04-19T01:52:38.809713Z"},"trusted":true},"outputs":[],"source":["!pip install einops\n","!pip install timm"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2022-04-19T01:54:11.881932Z","iopub.status.busy":"2022-04-19T01:54:11.881657Z","iopub.status.idle":"2022-04-19T01:54:11.977922Z","shell.execute_reply":"2022-04-19T01:54:11.977096Z","shell.execute_reply.started":"2022-04-19T01:54:11.881903Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","\n","from einops import rearrange\n","from timm.models.layers import DropPath, to_2tuple, trunc_normal_\n","\n","\n","class MLP(nn.Module):\n","    def __init__(self, in_features, hidden_features=None, out_features=None, drop=0.):\n","        super(MLP, self).__init__()\n","        out_features = out_features or in_features\n","        hidden_features = hidden_features or in_features\n","\n","        self.net = nn.Sequential(\n","            nn.Linear(in_features, hidden_features),\n","            nn.GELU(),\n","            nn.Dropout(drop),\n","            nn.Linear(hidden_features, out_features),\n","            nn.Dropout(drop)\n","        )\n","\n","    def forward(self, x):\n","        return self.net(x)\n","\n","\n","def window_partition(x, window_size):\n","    \"\"\"\n","    Args:\n","        x: (B, H, W, C)\n","        window_size (int): window size\n","    Returns:\n","        windows: (num_windows*B, window_size, window_size, C)\n","    \"\"\"\n","    B, H, W, C = x.shape\n","    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)\n","    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)\n","    return windows\n","\n","\n","def window_reverse(windows, window_size, H, W):\n","    \"\"\"\n","    Args:\n","        windows: (num_windows*B, window_size, window_size, C)\n","        window_size (int): Window size\n","        H (int): Height of image\n","        W (int): Width of image\n","    Returns:\n","        x: (B, H, W, C)\n","    \"\"\"\n","    B = int(windows.shape[0] / (H * W / window_size / window_size))\n","    x = windows.view(B, H // window_size, W // window_size, window_size, window_size, -1)\n","    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)\n","    return x\n","\n","\n","class WindowAttention(nn.Module):\n","    r\"\"\" Window based multi-head self attention (W-MSA) module with relative position bias.\n","    It supports both of shifted and non-shifted window.\n","    Args:\n","        dim (int): Number of input channels.\n","        window_size (tuple[int]): The height and width of the window.\n","        num_heads (int): Number of attention heads.\n","        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True\n","        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set\n","        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0\n","        proj_drop (float, optional): Dropout ratio of output. Default: 0.0\n","    \"\"\"\n","\n","    def __init__(self, dim, window_size, num_heads, qkv_bias=True, qk_scale=None, attn_drop=0., proj_drop=0.):\n","        super().__init__()\n","        self.dim = dim\n","        self.window_size = window_size  # Wh, Ww\n","        self.num_heads = num_heads\n","        head_dim = dim // num_heads\n","        self.scale = qk_scale or head_dim ** -0.5\n","\n","        # define a parameter table of relative position bias\n","        self.relative_position_bias_table = nn.Parameter(\n","            torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads))  # 2*Wh-1 * 2*Ww-1, nH\n","\n","        # get pair-wise relative position index for each token inside the window\n","        coords_h = torch.arange(self.window_size[0])\n","        coords_w = torch.arange(self.window_size[1])\n","        coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n","        coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n","        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n","        relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n","        relative_coords[:, :, 0] += self.window_size[0] - 1  # shift to start from 0\n","        relative_coords[:, :, 1] += self.window_size[1] - 1\n","        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n","        relative_position_index = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n","        self.register_buffer(\"relative_position_index\", relative_position_index)\n","\n","        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n","        self.attn_drop = nn.Dropout(attn_drop)\n","        self.proj = nn.Linear(dim, dim)\n","        self.proj_drop = nn.Dropout(proj_drop)\n","\n","        trunc_normal_(self.relative_position_bias_table, std=.02)\n","        self.softmax = nn.Softmax(dim=-1)\n","\n","    def forward(self, x, mask=None):\n","        \"\"\"\n","        Args:\n","            x: input features with shape of (num_windows*B, N, C)\n","            mask: (0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww) or None\n","        \"\"\"\n","        B_, N, C = x.shape\n","        qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n","        q, k, v = qkv[0], qkv[1], qkv[2]  # make torchscript happy (cannot use tensor as tuple)\n","\n","        q = q * self.scale\n","        attn = (q @ k.transpose(-2, -1))\n","\n","        relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(\n","            self.window_size[0] * self.window_size[1], self.window_size[0] * self.window_size[1], -1)  # Wh*Ww,Wh*Ww,nH\n","        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n","        attn = attn + relative_position_bias.unsqueeze(0)\n","\n","        if mask is not None:\n","            nW = mask.shape[0]\n","            attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)\n","            attn = attn.view(-1, self.num_heads, N, N)\n","            attn = self.softmax(attn)\n","        else:\n","            attn = self.softmax(attn)\n","\n","        attn = self.attn_drop(attn)\n","\n","        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)\n","        x = self.proj(x)\n","        x = self.proj_drop(x)\n","        return x\n","\n","\n","class SwinTransformerBlock(nn.Module):\n","    r\"\"\" Swin Transformer Block.\n","    Args:\n","        dim (int): Number of input channels.\n","        input_resolution (tuple[int]): Input resulotion.\n","        num_heads (int): Number of attention heads.\n","        window_size (int): Window size.\n","        shift_size (int): Shift size for SW-MSA.\n","        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n","        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n","        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n","        drop (float, optional): Dropout rate. Default: 0.0\n","        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n","        drop_path (float, optional): Stochastic depth rate. Default: 0.0\n","        act_layer (nn.Module, optional): Activation layer. Default: nn.GELU\n","        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n","    \"\"\"\n","\n","    def __init__(self, dim, input_resolution, num_heads, window_size=2, shift_size=0,\n","                 mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0., drop_path=0.,\n","                 act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n","        super().__init__()\n","        self.dim = dim\n","        self.input_resolution = input_resolution\n","        self.num_heads = num_heads\n","        self.window_size = window_size\n","        self.shift_size = shift_size\n","        self.mlp_ratio = mlp_ratio\n","        if min(self.input_resolution) <= self.window_size:\n","            # if window size is larger than input resolution, we don't partition windows\n","            self.shift_size = 0\n","            self.window_size = min(self.input_resolution)\n","        assert 0 <= self.shift_size < self.window_size, \"shift_size must in 0-window_size\"\n","\n","        self.norm1 = norm_layer(dim)\n","        self.attn = WindowAttention(\n","            dim, window_size=to_2tuple(self.window_size), num_heads=num_heads,\n","            qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n","\n","        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n","        self.norm2 = norm_layer(dim)\n","        mlp_hidden_dim = int(dim * mlp_ratio)\n","        self.mlp = MLP(in_features=dim, hidden_features=mlp_hidden_dim, drop=drop)\n","\n","        if self.shift_size > 0:\n","            # calculate attention mask for SW-MSA\n","            H, W = self.input_resolution\n","            img_mask = torch.zeros((1, H, W, 1))  # 1 H W 1\n","            h_slices = (slice(0, -self.window_size),\n","                        slice(-self.window_size, -self.shift_size),\n","                        slice(-self.shift_size, None))\n","            w_slices = (slice(0, -self.window_size),\n","                        slice(-self.window_size, -self.shift_size),\n","                        slice(-self.shift_size, None))\n","            cnt = 0\n","            for h in h_slices:\n","                for w in w_slices:\n","                    img_mask[:, h, w, :] = cnt\n","                    cnt += 1\n","\n","            mask_windows = window_partition(img_mask, self.window_size)  # nW, window_size, window_size, 1\n","            mask_windows = mask_windows.view(-1, self.window_size * self.window_size)\n","            attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)\n","            attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))\n","        else:\n","            attn_mask = None\n","\n","        self.register_buffer(\"attn_mask\", attn_mask)\n","\n","    def forward(self, x):\n","        H, W = self.input_resolution\n","        B, L, C = x.shape\n","        assert L == H * W, \"input feature has wrong size\"\n","\n","        shortcut = x\n","        x = self.norm1(x)\n","        x = x.view(B, H, W, C)\n","\n","        # cyclic shift\n","        if self.shift_size > 0:\n","            shifted_x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))\n","        else:\n","            shifted_x = x\n","\n","        # partition windows\n","        x_windows = window_partition(shifted_x, self.window_size)  # nW*B, window_size, window_size, C\n","        x_windows = x_windows.view(-1, self.window_size * self.window_size, C)  # nW*B, window_size*window_size, C\n","\n","        # W-MSA/SW-MSA\n","        attn_windows = self.attn(x_windows, mask=self.attn_mask)  # nW*B, window_size*window_size, C\n","\n","        # merge windows\n","        attn_windows = attn_windows.view(-1, self.window_size, self.window_size, C)\n","        shifted_x = window_reverse(attn_windows, self.window_size, H, W)  # B H' W' C\n","\n","        # reverse cyclic shift\n","        if self.shift_size > 0:\n","            x = torch.roll(shifted_x, shifts=(self.shift_size, self.shift_size), dims=(1, 2))\n","        else:\n","            x = shifted_x\n","        x = x.view(B, H * W, C)\n","\n","        # FFN\n","        x = shortcut + self.drop_path(x)\n","        x = x + self.drop_path(self.mlp(self.norm2(x)))\n","\n","        return x\n","\n","\n","class PatchMerging(nn.Module):\n","    r\"\"\" Patch Merging Layer.\n","    Args:\n","        input_resolution (tuple[int]): Resolution of input feature.\n","        dim (int): Number of input channels.\n","        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n","    \"\"\"\n","\n","    def __init__(self, input_resolution, dim, norm_layer=nn.LayerNorm):\n","        super().__init__()\n","        self.input_resolution = input_resolution\n","        self.dim = dim\n","        self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)\n","        self.norm = norm_layer(4 * dim)\n","\n","    def forward(self, x):\n","        \"\"\"\n","        x: B, H*W, C\n","        \"\"\"\n","        H, W = self.input_resolution\n","        B, L, C = x.shape\n","        assert L == H * W, \"input feature has wrong size\"\n","        assert H % 2 == 0 and W % 2 == 0, f\"x size ({H}*{W}) are not even.\"\n","\n","        x = x.view(B, H, W, C)\n","\n","        x0 = x[:, 0::2, 0::2, :]  # B H/2 W/2 C\n","        x1 = x[:, 1::2, 0::2, :]  # B H/2 W/2 C\n","        x2 = x[:, 0::2, 1::2, :]  # B H/2 W/2 C\n","        x3 = x[:, 1::2, 1::2, :]  # B H/2 W/2 C\n","        x = torch.cat([x0, x1, x2, x3], -1)  # B H/2 W/2 4*C\n","        x = x.view(B, -1, 4 * C)  # B H/2*W/2 4*C\n","\n","        x = self.norm(x)\n","        x = self.reduction(x)\n","\n","        return x\n","\n","\n","class PatchExpand(nn.Module):\n","    def __init__(self, input_resolution, dim, dim_scale=2, norm_layer=nn.LayerNorm):\n","        super().__init__()\n","        self.input_resolution = input_resolution\n","        self.dim = dim\n","        self.expand = nn.Linear(dim, 2*dim, bias=False) if dim_scale==2 else nn.Identity()\n","        self.norm = norm_layer(dim // dim_scale)\n","\n","    def forward(self, x):\n","        \"\"\"\n","        x: B, H*W, C\n","        \"\"\"\n","        H, W = self.input_resolution\n","        x = self.expand(x)\n","        B, L, C = x.shape\n","        assert L == H * W, \"input feature has wrong size\"\n","\n","        x = x.view(B, H, W, C)\n","        x = rearrange(x, 'b h w (p1 p2 c) -> b (h p1) (w p2) c', p1=2, p2=2, c=C//4)\n","        x = x.view(B, -1, C // 4)\n","        x = self.norm(x)\n","\n","        return x\n","\n","\n","class PatchEmbed(nn.Module):\n","    r\"\"\" Image to Patch Embedding\n","    Args:\n","        img_size (int): Image size.  Default: 224.\n","        patch_size (int): Patch token size. Default: 4.\n","        in_chans (int): Number of input image channels. Default: 3.\n","        embed_dim (int): Number of linear projection output channels. Default: 96.\n","        norm_layer (nn.Module, optional): Normalization layer. Default: None\n","    \"\"\"\n","\n","    def __init__(self, img_size=32, patch_size=4, in_chans=3, embed_dim=96, norm_layer=nn.LayerNorm):\n","        super().__init__()\n","        img_size = to_2tuple(img_size)\n","        patch_size = to_2tuple(patch_size)\n","        patches_resolution = [img_size[0] // patch_size[0], img_size[1] // patch_size[1]]\n","        self.img_size = img_size\n","        self.patch_size = patch_size\n","        self.patches_resolution = patches_resolution\n","        self.num_patches = patches_resolution[0] * patches_resolution[1]\n","\n","        self.in_chans = in_chans\n","        self.embed_dim = embed_dim\n","\n","        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n","        if norm_layer is not None:\n","            self.norm = norm_layer(embed_dim)\n","        else:\n","            self.norm = None\n","\n","    def forward(self, x):\n","        B, C, H, W = x.shape\n","        # fixme look at relaxing size constraints\n","        assert H == self.img_size[0] and W == self.img_size[1], \\\n","            f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n","        x = self.proj(x).flatten(2).transpose(1, 2)  # B Ph*Pw C\n","        if self.norm is not None:\n","            x = self.norm(x)\n","\n","        return x\n","\n","\n","class TransformerEncoder(nn.Module):\n","    \"\"\" A basic Swin Transformer layer for one stage.\n","    Args:\n","        dim (int): Number of input channels.\n","        input_resolution (tuple[int]): Input resolution.\n","        depth (int): Number of blocks.\n","        num_heads (int): Number of attention heads.\n","        window_size (int): Local window size.\n","        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n","        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n","        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n","        drop (float, optional): Dropout rate. Default: 0.0\n","        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n","        norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm\n","        downsample (nn.Module | None, optional): Downsample layer at the end of the layer. Default: None\n","    \"\"\"\n","\n","    def __init__(self, dim, input_resolution, depth, num_heads=4, window_size=2,\n","                 mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0.,\n","                 drop_path=0., norm_layer=nn.LayerNorm):\n","        super().__init__()\n","        self.dim = dim\n","        self.input_resolution = input_resolution\n","        self.depth = depth\n","\n","        # build blocks\n","        self.blocks = nn.ModuleList([\n","            SwinTransformerBlock(dim=dim, input_resolution=input_resolution,\n","                                 num_heads=num_heads, window_size=window_size,\n","                                 shift_size=0 if (i % 2 == 0) else window_size // 2,\n","                                 mlp_ratio=mlp_ratio,\n","                                 qkv_bias=qkv_bias, qk_scale=qk_scale,\n","                                 drop=drop, attn_drop=attn_drop,\n","                                 drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,\n","                                 norm_layer=norm_layer)\n","            for i in range(depth)])\n","\n","    def forward(self, x):\n","        for block in self.blocks:\n","            x = block(x)\n","        return x\n","\n","\n","\n","class Generator(nn.Module):\n","    def __init__(self, depth=[4, 3, 2], initial_size=8, dim=96, heads=4, mlp_ratio=4, window_size=2):\n","        super(Generator, self).__init__()\n","        self.initial_size = initial_size\n","        self.dim = dim\n","        self.depth = depth\n","        self.heads = heads\n","        self.mlp_ratio = mlp_ratio\n","        self.window_size = window_size\n","\n","        # N, 1024 -> N, 8 * 8 * 96\n","        self.mlp = nn.Linear(128, (self.initial_size ** 2) * self.dim)\n","\n","        # N, 8 * 8, 96 -> N, 8 * 8, 96\n","        self.decode1 = TransformerEncoder(dim=dim, input_resolution=[self.initial_size, self.initial_size], depth=self.depth[0], \\\n","            num_heads=self.heads, window_size=self.window_size, mlp_ratio=mlp_ratio)\n","        # N, 8 * 8, 96 -> N, 16 * 16, 48\n","        self.up1 = PatchExpand([self.initial_size, self.initial_size], dim=dim)\n","\n","        # N, 16 * 16, 48 -> N, 16 * 16, 48\n","        self.decode2 = TransformerEncoder(dim=(dim // 2), input_resolution=[self.initial_size * 2, self.initial_size * 2], depth=self.depth[1], \\\n","            num_heads=self.heads, window_size=2, mlp_ratio=mlp_ratio)\n","        # N, 16 * 16, 48 -> N, 32 * 32, 24\n","        self.up2 = PatchExpand([self.initial_size * 2, self.initial_size * 2], dim=(dim // 2))\n","\n","        # N, 32 * 32, 24 -> N, 32 * 32, 24\n","        self.decode3 = TransformerEncoder(dim=(dim // 4), input_resolution=[self.initial_size * 4, self.initial_size * 4], depth=self.depth[2], \\\n","            num_heads=self.heads, window_size=2, mlp_ratio=mlp_ratio)\n","\n","        # N, 24, 32, 32 -> N, 3, 32, 32\n","        self.linear = nn.Sequential(nn.Conv2d(self.dim // 4, 3, 1, 1, 0))\n","\n","        self.apply(self._init_weights)\n","\n","    def _init_weights(self, m):\n","        if isinstance(m, nn.Linear):\n","            trunc_normal_(m.weight, std=.02)\n","            if isinstance(m, nn.Linear) and m.bias is not None:\n","                nn.init.constant_(m.bias, 0)\n","        elif isinstance(m, nn.LayerNorm):\n","            nn.init.constant_(m.bias, 0)\n","            nn.init.constant_(m.weight, 1.0)\n","\n","    def forward(self, noise):\n","        # N, 8 * 8 * 384 -> N, 8 * 8, 96\n","        x = self.mlp(noise).view(-1, self.initial_size ** 2, self.dim)\n","\n","        # N, 8 * 8, 96 -> N, 8 * 8, 96\n","        x = self.decode1(x)\n","        # N, 8 * 8, 96 -> N, 16 * 16, 48\n","        x = self.up1(x)\n","        # N, 16 * 16, 48 -> N, 16 * 16, 48\n","        x = self.decode2(x)\n","        # N, 16 * 16, 48 -> N, 32 * 32, 24\n","        x = self.up2(x)\n","        # N, 32 * 32, 24 -> N, 32 * 32, 24\n","        x = self.decode3(x)\n","        # N, 32 * 32, 24 -> N, 24, 32 * 32 -> N, 24, 32, 32 -> N, 3, 32, 32\n","        x = self.linear(x.permute(0, 2, 1).view(-1, self.dim // 4, self.initial_size * 4, self.initial_size * 4))\n","\n","        return x\n","\n","\n","class Discriminator(nn.Module):\n","    def __init__(self, diff_aug, image_size=32, patch_size=4, input_channels=3, dim=96, depth=[2, 2, 2], heads=4, \\\n","        mlp_ratio=4, drop_rate=0., window_size=2, num_classes=1):\n","        super(Discriminator, self).__init__()\n","\n","        if image_size % patch_size != 0:\n","            raise ValueError('Image size must be divisible by patch size.')\n","        # num_patches: 8 * 8\n","        num_patches = (image_size // patch_size) ** 2\n","        self.diff_aug = diff_aug\n","        self.patch_size = patch_size\n","        self.depth = depth\n","        self.dim = dim\n","        self.heads = heads\n","        self.window_size = window_size\n","\n","        # split image into non-overlapping patches\n","        # N, 3, 32, 32 -> N, 8 * 8, 96\n","        self.patch_embed = PatchEmbed(img_size=image_size, patch_size=patch_size, in_chans=input_channels, embed_dim=dim)\n","\n","        # absolute position embedding\n","        # N, 64, 96\n","        self.positional_embedding = nn.Parameter(torch.zeros(1, num_patches, dim))\n","        nn.init.trunc_normal_(self.positional_embedding, std=0.02)\n","        self.droprate = nn.Dropout(p=drop_rate)\n","\n","        # N, 8 * 8, 96 -> N, 8 * 8, 96\n","        self.encode1 = TransformerEncoder(dim=self.dim, input_resolution=[image_size // 4, image_size // 4], depth=self.depth[0], \\\n","            num_heads=self.heads, window_size=self.window_size, mlp_ratio=mlp_ratio)\n","        # N, 8 * 8, 96 -> N, 4 * 4, 192\n","        self.down1 = PatchMerging(input_resolution=[image_size // 4, image_size // 4], dim=self.dim)\n","\n","        # N, 4 * 4, 192 -> N, 4 * 4, 192\n","        self.encode2 = TransformerEncoder(dim=(self.dim * 2), input_resolution=[image_size // 8, image_size // 8], depth=self.depth[1], \\\n","            num_heads=self.heads, window_size=self.window_size, mlp_ratio=mlp_ratio)\n","        # N, 4 * 4, 192 -> N, 2 * 2, 384\n","        self.down2 = PatchMerging(input_resolution=[image_size // 8, image_size // 8], dim=(self.dim * 2))\n","\n","        # N, 2 * 2, 384 -> N, 2 * 2, 384\n","        self.encode3 = TransformerEncoder(dim=(self.dim * 4), input_resolution=[image_size // 16, image_size // 16], depth=self.depth[2], \\\n","            num_heads=self.heads, window_size=self.window_size, mlp_ratio=mlp_ratio)\n","\n","        # N, 2 * 2, 384 -> N, 1\n","        self.norm = nn.LayerNorm(dim * 4)\n","        self.out = nn.Linear(dim * 4, num_classes)\n","\n","        self.apply(self._init_weights)\n","\n","    def _init_weights(self, m):\n","        if isinstance(m, nn.Linear):\n","            nn.init.trunc_normal_(m.weight, std=.02)\n","            if isinstance(m, nn.Linear) and m.bias is not None:\n","                nn.init.constant_(m.bias, 0)\n","        elif isinstance(m, nn.LayerNorm):\n","            nn.init.constant_(m.bias, 0)\n","            nn.init.constant_(m.weight, 1.0)\n","\n","    def forward(self, x):\n","        # data augmentation \n","        x = DiffAugment(x, self.diff_aug)\n","\n","        # N, 64, 96\n","        x = self.patch_embed(x)\n","        x += self.positional_embedding\n","        x = self.droprate(x)\n","\n","        # N, 8 * 8, 96 -> N, 8 * 8, 96\n","        x = self.encode1(x)\n","        # N, 8 * 8, 96 -> N, 4 * 4, 192\n","        x = self.down1(x)\n","        # N, 4 * 4, 192 -> N, 4 * 4, 192\n","        x = self.encode2(x)\n","        # N, 4 * 4, 192 -> N, 2 * 2, 384\n","        x = self.down2(x)\n","        # N, 2 * 2, 384 -> N, 2 * 2, 384\n","        x = self.encode3(x)\n","        # N, 2 * 2, 384 -> N, 2 * 2, 384\n","        x = self.norm(x)\n","        # N, 2 * 2, 384 -> N, 1\n","        x = self.out(x[:, 0])\n","        \n","        return x"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2022-04-19T01:54:27.368053Z","iopub.status.busy":"2022-04-19T01:54:27.367737Z","iopub.status.idle":"2022-04-19T01:54:27.389845Z","shell.execute_reply":"2022-04-19T01:54:27.388927Z","shell.execute_reply.started":"2022-04-19T01:54:27.368019Z"},"trusted":true},"outputs":[],"source":["import torch\n","import random\n","\n","import numpy as np\n","import torch.backends.cudnn as cudnn\n","\n","\n","def noise(imgs, latent_dim):\n","    return torch.FloatTensor(np.random.normal(0, 1, (imgs.shape[0], latent_dim)))\n","\n","def gener_noise(gener_batch_size, latent_dim):\n","    return torch.FloatTensor(np.random.normal(0, 1, (gener_batch_size, latent_dim)))\n","\n","def calculate_gradient_penalty(model, real_images, fake_images, constant=1.0, lamb=10.0):\n","    \"\"\"Calculates the gradient penalty loss for WGAN GP\"\"\"\n","    # Random weight term for interpolation between real and fake data\n","    alpha = torch.Tensor(np.random.random((real_images.size(0), 1, 1, 1))).to(real_images.get_device())\n","    # Get random interpolation between real and fake data\n","    interpolates = (alpha * real_images + ((1 - alpha) * fake_images)).requires_grad_(True)\n","\n","    model_interpolates = model(interpolates)\n","    grad_outputs = torch.ones([real_images.shape[0], 1], requires_grad=False).to(real_images.get_device())\n","\n","    # Get gradient w.r.t. interpolates\n","    gradients = torch.autograd.grad(\n","        outputs=model_interpolates,\n","        inputs=interpolates,\n","        grad_outputs=grad_outputs,\n","        create_graph=True,\n","        retain_graph=True,\n","        only_inputs=True,\n","    )[0]\n","    gradients = gradients.contiguous().view(gradients.size(0), -1)\n","    gradient_penalty = (((gradients + 1e-16).norm(2, dim=1) - constant) ** 2).mean() * lamb\n","    \n","    return gradient_penalty\n","\n","# wgan_loss\n","def wgan_loss(pred, real_or_not=True):\n","    if real_or_not:\n","        return - torch.mean(pred)\n","    else:\n","        return torch.mean(pred)\n","\n","\n","# Source from \"https://github.com/ultralytics/yolov5/blob/master/utils/torch_utils.py\"\n","def init_torch_seeds(seed: int = 0):\n","    r\"\"\" Sets the seed for generating random numbers. Returns a\n","    Args:\n","        seed (int): The desired seed.\n","    \"\"\"\n","    # Speed-reproducibility tradeoff https://pytorch.org/docs/stable/notes/randomness.html\n","    if seed == 0:  # slower, more reproducible\n","        cudnn.deterministic = True\n","        cudnn.benchmark = False\n","    else:  # faster, less reproducible\n","        cudnn.deterministic = False\n","        cudnn.benchmark = True\n","\n","    print(\"Initialize random seed.\")\n","    np.random.seed(seed)\n","    random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2022-04-19T01:53:04.938508Z","iopub.status.busy":"2022-04-19T01:53:04.938110Z","iopub.status.idle":"2022-04-19T01:53:04.952075Z","shell.execute_reply":"2022-04-19T01:53:04.951301Z","shell.execute_reply.started":"2022-04-19T01:53:04.938472Z"},"trusted":true},"outputs":[],"source":["from torchvision import transforms\n","from torch.utils.data import Dataset\n","\n","import torch\n","from PIL import Image\n","import os\n","\n","class Dataset(Dataset):\n","    def __init__(self, root, transform, mode='train'):\n","        super(Dataset, self).__init__()\n","\n","        self.root = root\n","        self.transform = transform\n","        self.mode = mode\n","\n","        data_dir = os.path.join(root, mode)\n","        self.file_list = os.listdir(data_dir)\n","\n","    def __len__(self):\n","        return len(self.file_list)\n","\n","    def __getitem__(self, index):\n","        img_path = os.path.join(self.root, self.mode, self.file_list[index])\n","        img = Image.open(img_path)\n","        img_out = self.transform(img)\n","\n","        return img_out\n","\n","\n","def data_loader(root, batch_size=20, shuffle=True, img_size=32, mode='train'):    \n","    transform = transforms.Compose([\n","        transforms.Resize(size=(img_size, img_size)),\n","        transforms.RandomHorizontalFlip(),\n","        transforms.ToTensor(),\n","        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n","        ])\n","    \n","    dset = Dataset(root, transform, mode=mode)\n","    \n","    if batch_size == 'all':\n","        batch_size = len(dset)\n","        \n","    dloader = torch.utils.data.DataLoader(dset,\n","                                          batch_size=batch_size,\n","                                          shuffle=shuffle,\n","                                          num_workers=0,\n","                                          drop_last=True)\n","    dlen = len(dset)\n","    \n","    return dloader, dlen"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2022-04-19T01:54:33.904088Z","iopub.status.busy":"2022-04-19T01:54:33.903828Z","iopub.status.idle":"2022-04-19T01:54:33.941893Z","shell.execute_reply":"2022-04-19T01:54:33.941025Z","shell.execute_reply.started":"2022-04-19T01:54:33.904059Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torch.optim as optim\n","from torchvision.utils import make_grid, save_image\n","\n","from tensorboardX import SummaryWriter\n","import time\n","import datetime\n","from tqdm import tqdm\n","\n","# for reproductionary\n","init_torch_seeds(seed=12345)\n","\n","class Solver():\n","    def __init__(self, root='dataset/anime_faces', result_dir='result', img_size=32, weight_dir='weight', load_weight=False,\n","                 batch_size=32, gener_batch_size=25, epochs=200, save_every=100, latent_dim=1024, n_critic=5, diff_aug=None, \n","                 g_lr=0.0002, d_lr=0.0001, beta_1=0.5, beta_2=0.999, logdir=None):\n","        \n","        # cpu or gpu\n","        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","        # load generator and discriminator\n","        self.G = Generator()\n","        self.G.to(self.device)\n","        self.D = Discriminator(diff_aug=diff_aug)\n","        self.D.to(self.device)\n","\n","        # load training dataset\n","        self.train_loader, _ = data_loader(root=root, batch_size=batch_size, shuffle=True, \n","                                                img_size=img_size, mode='train')\n","\n","        # optimizer\n","        self.optim_D = optim.Adam(self.D.parameters(), lr=d_lr, betas=(beta_1, beta_2))\n","        self.optim_G = optim.Adam(self.G.parameters(), lr=g_lr, betas=(beta_1, beta_2))\n","\n","        # Some hyperparameters\n","        self.latent_dim = latent_dim\n","        self.writer = SummaryWriter(logdir)\n","\n","        # Extra things\n","        self.result_dir = result_dir\n","        self.weight_dir = weight_dir\n","        self.load_weight = load_weight\n","        self.epochs = epochs\n","        self.gener_batch_size = gener_batch_size\n","        self.img_size = img_size\n","        self.start_epoch = 0\n","        self.num_epoch = epochs\n","        self.save_every = save_every\n","        self.n_critic = n_critic\n","        \n","    '''\n","    <show_model >\n","    Print model architectures\n","    '''\n","    def show_model(self):\n","        print('================================ Discriminator for image =====================================')\n","        print(self.D)\n","        print('==========================================================================================\\n\\n')\n","        print('================================= Generator ==================================================')\n","        print(self.G)\n","        print('==========================================================================================\\n\\n')\n","        \n","    '''\n","        < set_train_phase >\n","        Set training phase\n","    '''\n","    def set_train_phase(self):\n","        self.D.train()\n","        self.G.train()\n","    \n","    '''\n","        < load_checkpoint >\n","        If you want to continue to train, load pretrained weight from checkpoint\n","    '''\n","    def load_checkpoint(self, checkpoint):\n","        print('Load model')\n","        self.D.load_state_dict(checkpoint['discriminator_image_state_dict'])\n","        self.G.load_state_dict(checkpoint['generator_state_dict'])\n","        self.optim_D.load_state_dict(checkpoint['optim_d'])\n","        self.optim_G.load_state_dict(checkpoint['optim_g'])\n","        self.start_epoch = checkpoint['epoch']\n","        \n","    '''\n","        < save_checkpoint >\n","        Save checkpoint\n","    '''\n","    def save_checkpoint(self, state, file_name):\n","        print('saving check_point')\n","        torch.save(state, file_name)\n","    \n","    '''\n","        < all_zero_grad >\n","        Set all optimizers' grad to zero \n","    '''\n","    def all_zero_grad(self):\n","        self.optim_D.zero_grad()\n","        self.optim_G.zero_grad()\n","\n","    '''\n","        < train >\n","        Train the D_image, D_latnet, G and E \n","    '''\n","    def train(self):\n","        if self.load_weight is True:\n","            weight_name = 'checkpoint_{epoch}_epoch.pkl'.format(epoch=self.epochs)\n","            checkpoint = torch.load(os.path.join(self.weight_dir, weight_name))\n","            self.load_checkpoint(checkpoint)\n","        \n","        self.set_train_phase()\n","        self.show_model()\n","\n","        print('====================     Training    Start... =====================')\n","        for epoch in range(self.start_epoch, self.num_epoch):\n","            start_time = time.time()\n","\n","            for iters, img in tqdm(enumerate(self.train_loader)):\n","                # load real images\n","                real_imgs = img.type(torch.cuda.FloatTensor)\n","\n","                # generate fake images\n","                noise = torch.cuda.FloatTensor(np.random.normal(0, 1, (img.shape[0], self.latent_dim)))\n","                fake_imgs = self.G(noise)\n","\n","                ''' ----------------------------- 1. Train D ----------------------------- '''\n","                # discriminator predict \n","                fake_validity = self.D(fake_imgs.detach())\n","                real_validity = self.D(real_imgs)\n","\n","                # gradient_penalty\n","                gradient_penalty = calculate_gradient_penalty(self.D, real_imgs, fake_imgs.detach())\n","                # loss measures generator's ability to fool the discriminator\n","                err_d = wgan_loss(real_validity, real_or_not=True) + wgan_loss(fake_validity, real_or_not=False) \n","\n","                d_loss = gradient_penalty + err_d\n","                self.writer.add_scalars('losses', {'d_loss': d_loss, 'grad_penalty': gradient_penalty}, iters)\n","\n","                # update D\n","                self.all_zero_grad()\n","                d_loss.backward()\n","                self.optim_D.step()\n","\n","                ''' ----------------------------- 2. Train G ----------------------------- '''\n","                # train the generator every n_critic iterations\n","                if iters % self.n_critic == 0:\n","                    noise = torch.cuda.FloatTensor(np.random.normal(0, 1, (img.shape[0], self.latent_dim)))\n","                    generated_imgs = self.G(noise)\n","                    \n","                    fake_validity = self.D(generated_imgs)\n","                    g_loss = wgan_loss(fake_validity, real_or_not=True).to(self.device)\n","\n","                    self.writer.add_scalars('losses', {'g_loss': g_loss}, iters)\n","\n","                    # update G\n","                    self.all_zero_grad()\n","                    g_loss.backward()\n","                    self.optim_G.step()\n","\n","                log_file = open('log.txt', 'w')\n","                log_file.write(str(epoch))\n","\n","                # Print error, save intermediate result image and weight\n","                if epoch and iters % self.save_every == 0:\n","                    et = time.time() - start_time\n","                    et = str(datetime.timedelta(seconds=et))[:-7]\n","                    print('[Elapsed : %s /Epoch : %d / Iters : %d] => loss_d : %f / loss_g : %f / gradient_penalty : %f '\\\n","                          %(et, epoch, iters, d_loss.item(), g_loss.item(), gradient_penalty.item()))\n","\n","                    # Save intermediate result image\n","                    if os.path.exists(self.result_dir) is False:\n","                        os.makedirs(self.result_dir)\n","\n","                    self.G.eval()\n","                    with torch.no_grad():\n","                        gene_noise = torch.cuda.FloatTensor(np.random.normal(0, 1, (self.gener_batch_size, self.latent_dim)))\n","                        generated_imgs= self.G(gene_noise)\n","                        sample_imgs = generated_imgs[:25]\n","\n","                    img_name = 'generated_img_{epoch}_{iters}.jpg'.format(epoch=epoch, iters=(iters % len(self.train_loader)))\n","                    img_path = os.path.join(self.result_dir, img_name)\n","\n","                    img_grid = make_grid(sample_imgs, nrow=5, normalize=True, scale_each=True)\n","                    save_image(img_grid, img_path, nrow=5, normalize=True, scale_each=True)  \n","\n","                    # save intermediate weight\n","                    if os.path.exists(self.weight_dir) is False:\n","                        os.makedirs(self.weight_dir)  \n","\n","            # Save weight at the end of every epoch\n","            if (epoch % 25) == 0:\n","                # self.save_weight(epoch=epoch)\n","                checkpoint = {\n","                    \"generator_state_dict\": self.G.state_dict(),\n","                    \"discriminator_image_state_dict\": self.D.state_dict(),\n","                    \"optim_g\": self.optim_G.state_dict(),\n","                    \"optim_d\": self.optim_D.state_dict(),\n","                    \"epoch\": epoch\n","                    }\n","                path_checkpoint = os.path.join(self.weight_dir, \"checkpoint_{}_epoch.pkl\".format(epoch))\n","                self.save_checkpoint(checkpoint, path_checkpoint)        "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-19T01:54:38.979539Z","iopub.status.busy":"2022-04-19T01:54:38.979155Z"},"trusted":true},"outputs":[],"source":["import argparse\n","import os\n","\n","def main(args):\n","    solver = Solver(root = args.root,\n","                    result_dir = args.result_dir,\n","                    img_size = args.img_size,\n","                    weight_dir = args.weight_dir,\n","                    batch_size = args.batch_size,\n","                    gener_batch_size = args.gener_batch_size,\n","                    g_lr = args.g_lr,\n","                    d_lr = args.d_lr,\n","                    beta_1 = args.beta_1,\n","                    beta_2 = args.beta_2,\n","                    save_every = args.save_every,\n","                    latent_dim = args.latent_dim,\n","                    n_critic = args.n_critic,\n","                    epochs = args.epochs,\n","                    diff_aug = args.diff_aug,\n","                    load_weight = args.load_weight,\n","                    )\n","                    \n","    solver.train()\n","\n","if __name__ == '__main__':\n","    parser = argparse.ArgumentParser()\n","\n","    parser.add_argument('--root', type=str, default='../input/anime64', help='Data location')\n","    parser.add_argument('--result_dir', type=str, default='./', help='Result images location')\n","    parser.add_argument('--img_size', type=int, default=32, help='Size of image for discriminator input.')\n","    parser.add_argument('--weight_dir', type=str, default='./', help='Weight location')\n","    parser.add_argument('--batch_size', type=int, default=50, help='Training batch size')\n","    parser.add_argument('--gener_batch_size', type=int, default=25, help='Batch size for generator.')\n","    parser.add_argument('--g_lr', type=float, default=0.0001, help='Learning rate')\n","    parser.add_argument('--d_lr', type=float, default=0.0002, help='Discriminator Learning rate')\n","    parser.add_argument('--beta_1', type=float, default=0.0, help='Beta1 for Adam')\n","    parser.add_argument('--beta_2', type=float, default=0.99, help='Beta2 for Adam')\n","    parser.add_argument('--save_every', type=int, default=200, help='How often do you want to see the result?')\n","    parser.add_argument('--latent_dim', type=int, default=128, help='Latent dimension.')\n","    parser.add_argument('--n_critic', type=int, default=5, help='n_critic.')\n","    parser.add_argument('--epochs', type=int, default=1000, help='Number of epoch.')\n","    parser.add_argument('--diff_aug', type=str, default=\"translation,cutout,color\", help='Data Augmentation')\n","    parser.add_argument('--load_weight', type=bool, default=False, help='Load weight or not')\n","                        \n","    args = parser.parse_args([])\n","    main(args=args)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":4}
